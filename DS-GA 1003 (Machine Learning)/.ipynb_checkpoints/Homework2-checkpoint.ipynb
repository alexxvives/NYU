{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22187d13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:43.157070Z",
     "start_time": "2022-02-09T20:34:41.039978Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf2284",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b985717a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:43.222091Z",
     "start_time": "2022-02-09T20:34:43.205511Z"
    }
   },
   "outputs": [],
   "source": [
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array of size(num_instances, num_features)\n",
    "        test - test set, a 2D numpy array of size(num_instances, num_features)\n",
    "\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized - test set after normalization\n",
    "    \"\"\"\n",
    "            \n",
    "    train = pd.DataFrame(train)    \n",
    "    test = pd.DataFrame(test)   \n",
    "    \n",
    "    for feature in train.columns:\n",
    "        if train[feature].nunique() == 1:\n",
    "            train.drop(feature, axis=1)\n",
    "            test.drop(feature, axis=1)\n",
    "            \n",
    "    min_val = np.min(train, axis=0)\n",
    "    max_val = np.max(train, axis=0)\n",
    "    \n",
    "    train = (train - min_val) / (max_val - min_val)\n",
    "    test = (test - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return np.array(train), np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a196a7ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:43.320148Z",
     "start_time": "2022-02-09T20:34:43.272684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the dataset\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ridge_regression_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3276/1741401214.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3276/1741401214.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loading the dataset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ridge_regression_dataset.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ridge_regression_dataset.csv'"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    #Loading the dataset\n",
    "    print('loading the dataset')\n",
    "\n",
    "    df = pd.read_csv('ridge_regression_dataset.csv', delimiter=',')\n",
    "    X = df.values[:,:-1]\n",
    "    y = df.values[:,-1]\n",
    "\n",
    "    print('Split into Train and Test')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=10)\n",
    "\n",
    "    print(\"Scaling all to [0, 1]\")\n",
    "    X_train, X_test = feature_normalization(X_train, X_test)\n",
    "    X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))  # Add bias term\n",
    "    X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80acea11",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00729233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:43.384144Z",
     "start_time": "2022-02-09T20:34:43.368495Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the average square loss for predicting y with X*theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D array of size(num_features)\n",
    "\n",
    "    Returns:\n",
    "        loss - the average square loss, scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = (1 / X.shape[0]) * np.sum((X @ theta - y)**2)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b62494",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f46d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:43.448065Z",
     "start_time": "2022-02-09T20:34:43.432106Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the average square loss(as defined in compute_square_loss), at the point theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size(num_features)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size(num_features)\n",
    "    \"\"\"\n",
    "    \n",
    "    grad = (2 / X.shape[0]) * (X.T @ X @ theta - X.T @ y)\n",
    "    \n",
    "    return grad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97729646",
   "metadata": {},
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec00028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:43.512055Z",
     "start_time": "2022-02-09T20:34:43.496062Z"
    }
   },
   "outputs": [],
   "source": [
    "#Getting the gradient calculation correct is often the trickiest part of any gradient-based optimization algorithm. \n",
    "#Fortunately, it's very easy to check that the gradient calculation is correct using the definition of gradient.\n",
    "#See http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"Implement Gradient Checker\n",
    "    Check that the function compute_square_loss_gradient returns the\n",
    "    correct gradient for the given X, y, and theta.\n",
    "\n",
    "    Let d be the number of features. Here we numerically estimate the\n",
    "    gradient by approximating the directional derivative in each of\n",
    "    the d coordinate directions:\n",
    "(e_1 =(1,0,0,...,0), e_2 =(0,1,0,...,0), ..., e_d =(0,...,0,1))\n",
    "\n",
    "    The approximation for the directional derivative of J at the point\n",
    "    theta in the direction e_i is given by:\n",
    "(J(theta + epsilon * e_i) - J(theta - epsilon * e_i)) /(2*epsilon).\n",
    "\n",
    "    We then look at the Euclidean distance between the gradient\n",
    "    computed using this approximation and the gradient computed by\n",
    "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
    "    distance exceeds tolerance, we say the gradient is incorrect.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size(num_features)\n",
    "        epsilon - the epsilon used in approximation\n",
    "        tolerance - the tolerance error\n",
    "\n",
    "    Return:\n",
    "        A boolean value indicating whether the gradient is correct or not\n",
    "    \"\"\"\n",
    "    \n",
    "    true_gradient = compute_square_loss_gradient(X, y, theta) #The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    \n",
    "    I = np.identity(theta.shape[0])\n",
    "    \n",
    "    for i in range(0, num_features):\n",
    "        approx_grad[i] = (compute_square_loss(X, y, theta + epsilon * I[:,i]) - compute_square_loss(X, y, theta - epsilon * I[:,i]))/(2*epsilon)\n",
    "    distance = np.sqrt(np.sum((true_gradient - approx_grad)**2))\n",
    "    \n",
    "    return distance\n",
    "    if distance < tolerance: \n",
    "        return True #There is no major difference between gradients\n",
    "    else:\n",
    "        return False #There is a major difference between gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3129e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:44.069758Z",
     "start_time": "2022-02-09T20:34:44.042590Z"
    }
   },
   "outputs": [],
   "source": [
    "def generic_gradient_checker(X, y, theta, objective_func, gradient_func, \n",
    "                             epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    The functions takes objective_func and gradient_func as parameters. \n",
    "    And check whether gradient_func(X, y, theta) returned the true \n",
    "    gradient for objective_func(X, y, theta).\n",
    "    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient\n",
    "    \"\"\"\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640fd2c",
   "metadata": {},
   "source": [
    "### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f41c76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:44.853739Z",
     "start_time": "2022-02-09T20:34:44.825882Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_grad_descent(X, y, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement batch gradient descent to\n",
    "    minimize the average square loss objective.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_step - number of steps to run\n",
    "        grad_check - a boolean value indicating whether checking the gradient when updating\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size(num_step+1, num_features)\n",
    "                     for instance, theta in step 0 should be theta_hist[0], theta in step(num_step) is theta_hist[-1]\n",
    "        loss_hist - the history of average square loss on the data, 1D numpy array,(num_step+1)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_step + 1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step + 1)  #Initialize loss_hist\n",
    "    theta = np.zeros(num_features)  #Initialize theta\n",
    "    \n",
    "    loss_hist[0] = compute_square_loss(X, y, theta)\n",
    "    theta_hist[0] = theta\n",
    "    for i in range(1, num_step+1):\n",
    "    \n",
    "        theta = theta - alpha * compute_square_loss_gradient(X, y, theta)\n",
    "            \n",
    "        loss = compute_square_loss(X, y, theta)\n",
    "        loss_hist[i] = loss\n",
    "        theta_hist[i] = theta\n",
    "        \n",
    "    return theta_hist, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69fced9",
   "metadata": {},
   "source": [
    "### Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97615d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:46.440712Z",
     "start_time": "2022-02-09T20:34:46.015760Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1001, 1001)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "theta_hist, loss_hist = batch_grad_descent(X_train, y_train, alpha=0.01, num_step=1000, grad_check=False)\n",
    "sns.lineplot(x=x, y=loss_hist, label='alpha = 0.01')\n",
    "\n",
    "theta_hist, loss_hist = batch_grad_descent(X_train, y_train, alpha=0.05, num_step=1000, grad_check=False)\n",
    "sns.lineplot(x=x, y=loss_hist, label='alpha = 0.05')\n",
    "\n",
    "#theta_hist, loss_hist = batch_grad_descent(X_train, y_train, alpha=0.5, num_step=1000, grad_check=False)\n",
    "#sns.lineplot(x=x, y=theta_hist[:,0], label='alpha = 0.5')\n",
    "\n",
    "#theta_hist, loss_hist = batch_grad_descent(X_train, y_train, alpha=0.1, num_step=1000, grad_check=False)\n",
    "#sns.lineplot(x=x, y=theta_hist[:,0], label='alpha = 0.5')\n",
    "\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('training_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0ca084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-07T22:14:26.236579Z",
     "start_time": "2022-02-07T22:14:26.220546Z"
    }
   },
   "source": [
    "The step sizes 0.01 and 0.05 converge but 0.1 and 0.5 diverge.\n",
    "Additionally, we can see that the minimum loss is around 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b8353",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:13:44.302459Z",
     "start_time": "2022-02-09T02:13:44.292513Z"
    }
   },
   "source": [
    "### Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ce8d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:48.361571Z",
     "start_time": "2022-02-09T20:34:47.664812Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "theta_hist, loss_hist = batch_grad_descent(X_train, y_train, alpha=0.05, num_step=1000, grad_check=False)\n",
    "\n",
    "test_loss = []\n",
    "for i in range(len(theta_hist)):\n",
    "    test_loss.append(compute_square_loss(X_test, y_test, theta_hist[i]))\n",
    "\n",
    "plt.xscale('log')   \n",
    "plt.plot(x, loss_hist, label='TRAIN', color='black')\n",
    "plt.plot(x, test_loss, label='TEST', color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4737da14",
   "metadata": {},
   "source": [
    "### Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd3bad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:49.925470Z",
     "start_time": "2022-02-09T20:34:49.915658Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute the gradient of L2-regularized average square loss function given X, y and theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size(num_features)\n",
    "        lambda_reg - the regularization coefficient\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size(num_features)\n",
    "    \"\"\"\n",
    "    grad = (2 / X.shape[0]) * (X.T @ X @ theta - X.T @ y) + 2 * lambda_reg * theta\n",
    "    \n",
    "    return grad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e6cc72",
   "metadata": {},
   "source": [
    "### Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693f4ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:34:50.629736Z",
     "start_time": "2022-02-09T20:34:50.605752Z"
    }
   },
   "outputs": [],
   "source": [
    "def regularized_grad_descent(X, y, alpha=0.05, lambda_reg=10**-2, num_step=1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_step - number of steps to run\n",
    "    \n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size(num_step+1, num_features)\n",
    "                     for instance, theta in step 0 should be theta_hist[0], theta in step(num_step+1) is theta_hist[-1]\n",
    "        loss hist - the history of average square loss function without the regularization term, 1D numpy array.\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta = np.zeros(num_features) #Initialize theta\n",
    "    theta_hist = np.zeros((num_step+1, num_features)) #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step+1) #Initialize loss_hist\n",
    "\n",
    "    loss_hist[0] = compute_square_loss(X, y, theta)\n",
    "    theta_hist[0] = theta\n",
    "    for i in range(1, num_step+1):\n",
    "    \n",
    "        theta = theta - alpha * compute_regularized_square_loss_gradient(X, y, theta, lambda_reg)\n",
    "        loss = compute_square_loss(X, y, theta)\n",
    "        loss_hist[i] = loss\n",
    "        theta_hist[i] = theta\n",
    "        \n",
    "    return theta_hist, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b061c06",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc43fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T20:45:25.879180Z",
     "start_time": "2022-02-09T20:45:25.852408Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_test_loss(X_train, y_train, X_test, y_test, lambda_reg):\n",
    "\n",
    "    theta_hist, loss_hist = regularized_grad_descent(X_train, y_train, lambda_reg=lambda_reg, num_step=1000)\n",
    "    test_loss = []\n",
    "    for i in range(len(theta_hist)):\n",
    "        test_loss.append(compute_square_loss(X_test, y_test, theta_hist[i]))\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a2c7e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T21:50:42.540033Z",
     "start_time": "2022-02-09T21:50:41.487126Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "theta_hist, loss_hist_1 = regularized_grad_descent(X_train, y_train, alpha=0.05, lambda_reg=10**-7, num_step=1000)\n",
    "theta_hist, loss_hist_2 = regularized_grad_descent(X_train, y_train, alpha=0.05, lambda_reg=10**-5, num_step=1000)\n",
    "theta_hist, loss_hist_3 = regularized_grad_descent(X_train, y_train, alpha=0.05, lambda_reg=10**-3, num_step=1000)\n",
    "theta_hist, loss_hist_4 = regularized_grad_descent(X_train, y_train, alpha=0.05, lambda_reg=0.01, num_step=1000)\n",
    "theta_hist, loss_hist_5 = regularized_grad_descent(X_train, y_train, alpha=0.05, lambda_reg=0.1, num_step=1000)\n",
    "theta_hist, loss_hist_6 = regularized_grad_descent(X_train, y_train, alpha=0.05, lambda_reg=10, num_step=1000)\n",
    "theta_hist, loss_hist_7 = regularized_grad_descent(X_train, y_train, alpha=0.05, lambda_reg=100, num_step=1000)\n",
    "\n",
    "sns.lineplot(x=x, y=loss_hist_1, label='lambda 10^-7')\n",
    "sns.lineplot(x=x, y=loss_hist_2, label='lambda 10^-5')\n",
    "sns.lineplot(x=x, y=loss_hist_3, label='lambda 10^-3')\n",
    "sns.lineplot(x=x, y=loss_hist_4, label='lambda 0.01')\n",
    "sns.lineplot(x=x, y=loss_hist_5, label='lambda 0.1')\n",
    "sns.lineplot(x=x, y=loss_hist_6, label='lambda 10')\n",
    "sns.lineplot(x=x, y=loss_hist_6, label='lambda 100')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Training error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eff99d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T21:50:43.976672Z",
     "start_time": "2022-02-09T21:50:42.667538Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "test_loss_1 = plot_test_loss(X_train, y_train, X_test, y_test, lambda_reg=10**-7)\n",
    "test_loss_2 = plot_test_loss(X_train, y_train, X_test, y_test, lambda_reg=10**-5)\n",
    "test_loss_3 = plot_test_loss(X_train, y_train, X_test, y_test, lambda_reg=10**-3)\n",
    "test_loss_4 = plot_test_loss(X_train, y_train, X_test, y_test, lambda_reg=0.01)\n",
    "test_loss_5 = plot_test_loss(X_train, y_train, X_test, y_test, lambda_reg=0.1)\n",
    "test_loss_6 = plot_test_loss(X_train, y_train, X_test, y_test, lambda_reg=10)\n",
    "test_loss_7 = plot_test_loss(X_train, y_train, X_test, y_test, lambda_reg=100)\n",
    "\n",
    "sns.lineplot(x=x, y=test_loss_1, label='lambda 10^-7')\n",
    "sns.lineplot(x=x, y=test_loss_2, label='lambda 10^-5')\n",
    "sns.lineplot(x=x, y=test_loss_3, label='lambda 10^-3')\n",
    "sns.lineplot(x=x, y=test_loss_4, label='lambda 0.01')\n",
    "sns.lineplot(x=x, y=test_loss_5, label='lambda 0.1')\n",
    "sns.lineplot(x=x, y=test_loss_6, label='lambda 10')\n",
    "sns.lineplot(x=x, y=test_loss_7, label='lambda 100')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log'),\n",
    "plt.title('Testing error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d74d52",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Overfitting consists on the training error being very low and the testing error being high, meaning that the training data was memorized and therefore the model fails to generalize. In this case, we can see that around step 400, the testing error starts increasing while the training error keeps going down, that when overfitting starts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71696c8e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Question 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b3117",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T21:50:50.753793Z",
     "start_time": "2022-02-09T21:50:50.212934Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "a = [loss_hist_1[-1], loss_hist_2[-1], loss_hist_3[-1], loss_hist_4[-1], loss_hist_5[-1]]\n",
    "b = [test_loss_1[-1], test_loss_2[-1], test_loss_3[-1], test_loss_4[-1], test_loss_5[-1]]\n",
    "lambdas = [10**-7, 10**-5, 10**-3, 0.01, 0.1]\n",
    "\n",
    "plt.plot(lambdas, a, label='Training loss', marker='s')\n",
    "plt.plot(lambdas, b, label='Testing loss', marker='s')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085c7ad",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The best lambda would be where the training error intersects the testing error. Therefore, out of these lambdas I would choose lambda=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454b47f",
   "metadata": {},
   "source": [
    "### Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376cb52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T21:51:34.492820Z",
     "start_time": "2022-02-09T21:51:34.023320Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "a = [loss_hist_1[-1], loss_hist_2[-1], loss_hist_3[-1], loss_hist_4[-1], loss_hist_5[-1]]\n",
    "b = [test_loss_1[-1], test_loss_2[-1], test_loss_3[-1], test_loss_4[-1], test_loss_5[-1]]\n",
    "min_a = [min(loss_hist_1), min(loss_hist_2), min(loss_hist_3), min(loss_hist_4), min(loss_hist_5)]\n",
    "min_b = [min(test_loss_1), min(test_loss_2), min(test_loss_3), min(test_loss_4), min(test_loss_5)]\n",
    "lambdas = [10**-7, 10**-5, 10**-3, 0.01, 0.1]\n",
    "\n",
    "plt.plot(lambdas, a, label='Training loss', marker='s')\n",
    "plt.plot(lambdas, b, label='Testing loss', marker='s')\n",
    "plt.plot(lambdas, min_a, label='min_train', marker='s')\n",
    "plt.plot(lambdas, min_b, label='min_test', marker='s')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3cc07",
   "metadata": {},
   "source": [
    "Now, the diference between testing errors is much lower so it does not matter as much but it does seem like the best lambda is still 0.01\n",
    "\n",
    "Additionally, due to the fact that the training error always decreases, the minimum loss in training error is the same as the last training error (the green and blue line overlap)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b352ac",
   "metadata": {},
   "source": [
    "### Question 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f88af",
   "metadata": {},
   "source": [
    "In practice, I would choose the theta that minimizes the test loss. In this case that is when alpha is 0.05, lambda is 0.01 and we use early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb7b52",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Question 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a68913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-09T02:13:08.625247Z",
     "start_time": "2022-02-09T02:13:08.611278Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def stochastic_grad_descent(X, y, alpha=0.01, lambda_reg=10**-2, num_epoch=1000, eta0=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement stochastic gradient descent with regularization term\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size(num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size(num_instances)\n",
    "        alpha - string or float, step size in gradient descent\n",
    "                NOTE: In SGD, it's not a good idea to use a fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
    "                if alpha is a float, then the step size in every step is the float.\n",
    "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t).\n",
    "                if alpha == \"1/t\", alpha = 1/t.\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_epoch - number of epochs to go through the whole training set\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 3D numpy array of size(num_epoch, num_instances, num_features)\n",
    "                     for instance, theta in epoch 0 should be theta_hist[0], theta in epoch(num_epoch) is theta_hist[-1]\n",
    "        loss hist - the history of loss function vector, 2D numpy array of size(num_epoch, num_instances)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta = np.ones(num_features) #Initialize theta\n",
    "    theta_hist = np.zeros((num_epoch, num_instances, num_features)) #Initialize theta_hist\n",
    "    loss_hist = np.zeros((num_epoch, num_instances)) #Initialize loss_hist\n",
    "    np.random.seed(27)\n",
    "    \n",
    "    for epoch in range(0, num_epoch):\n",
    "        \n",
    "        #random_index = np.random.choice(list(range(0, len(X))), len(X), replace=False)\n",
    "        random_index = np.random.permutation(num_instances)\n",
    "        for i in range(0, len(random_index)): \n",
    "            \n",
    "            #Save the historical loss and thetas\n",
    "            loss_hist[epoch, i] = compute_square_loss(X, y, theta) + lambda_reg * theta.T @ theta\n",
    "            theta_hist[epoch, i] = theta\n",
    "            \n",
    "            X_temp = X[random_index[i]].reshape(1,49)\n",
    "            y_temp = y[random_index[i]].reshape(1,1)\n",
    "            theta = theta.reshape(49,1)\n",
    "            \n",
    "            grad = 2 * (X_temp.T @ X_temp @ theta - (X_temp.T @ y_temp)) + 2 * lambda_reg * theta\n",
    "            \n",
    "            grad = grad.reshape(49,)\n",
    "            theta = theta.reshape(49,)\n",
    "            \n",
    "            t = epoch * 100 + i+1\n",
    "            #Check alpha\n",
    "            if alpha == \"1/sqrt(t)\": \n",
    "                theta = theta - 0.1/np.sqrt(t) * grad\n",
    "            elif alpha == \"1/t\":\n",
    "                theta = theta - 0.1/t * grad\n",
    "            else: \n",
    "                theta = theta - alpha * grad        \n",
    "            \n",
    "    return theta_hist, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d35e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "x = np.linspace(0, 1000, 1000)\n",
    "\n",
    "theta_hist1, loss_hist1 = stochastic_grad_descent(X_train, y_train, alpha=0.01, lambda_reg=10**-2, num_epoch=1000)\n",
    "theta_hist2, loss_hist2 = stochastic_grad_descent(X_train, y_train, alpha=0.005, lambda_reg=10**-2, num_epoch=1000)\n",
    "theta_hist3, loss_hist3 = stochastic_grad_descent(X_train, y_train, alpha=\"1/t\", lambda_reg=10**-2, num_epoch=1000)\n",
    "theta_hist4, loss_hist4 = stochastic_grad_descent(X_train, y_train, alpha=\"1/sqrt(t)\", lambda_reg=10**-2, num_epoch=1000)\n",
    "theta_hist5, loss_hist5 = stochastic_grad_descent(X_train, y_train, alpha=\"1/sqrt(t)\", lambda_reg=10**-2, num_epoch=1000)\n",
    "\n",
    "sns.lineplot(x=x, y=loss_hist1[:,-1], label='alpha = 0.01')\n",
    "sns.lineplot(x=x, y=loss_hist2[:,-1], label='alpha = 0.005')\n",
    "sns.lineplot(x=x, y=loss_hist3[:,-1], label='alpha = 0.1/t')\n",
    "sns.lineplot(x=x, y=loss_hist4[:,-1], label='alpha = 0.1/sqrt(t)')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Training error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112f5b3",
   "metadata": {},
   "source": [
    "### Question 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e34eee",
   "metadata": {},
   "source": [
    "It is clear that when the step size is constant there is much moire noise. Additionally, using a step size of C/sqrt(t) appears to be the best alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc969fdb",
   "metadata": {},
   "source": [
    "### Question 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae887dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_mnist_01():\n",
    "    \"\"\"\n",
    "    Load the mnist datasets, selects the classes 0 and 1 \n",
    "    and normalize the data.\n",
    "    Args: none\n",
    "    Outputs: \n",
    "        X_train: np.array of size (n_training_samples, n_features)\n",
    "        X_test: np.array of size (n_test_samples, n_features)\n",
    "        y_train: np.array of size (n_training_samples)\n",
    "        y_test: np.array of size (n_test_samples)\n",
    "    \"\"\"\n",
    "    X_mnist, y_mnist = fetch_openml('mnist_784', version=1, \n",
    "                                    return_X_y=True, as_frame=False)\n",
    "    indicator_01 = (y_mnist == '0') + (y_mnist == '1')\n",
    "    X_mnist_01 = X_mnist[indicator_01]\n",
    "    y_mnist_01 = y_mnist[indicator_01]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_mnist_01, y_mnist_01,\n",
    "                                                        test_size=0.33,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train) \n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    y_test = 2 * np.array([int(y) for y in y_test]) - 1\n",
    "    y_train = 2 * np.array([int(y) for y in y_train]) - 1\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d138731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_sample(N_train, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Subsample the training data to keep only N first elements\n",
    "    Args: none\n",
    "    Outputs: \n",
    "        X_train: np.array of size (n_training_samples, n_features)\n",
    "        X_test: np.array of size (n_test_samples, n_features)\n",
    "        y_train: np.array of size (n_training_samples)\n",
    "        y_test: np.array of size (n_test_samples)\n",
    "    \"\"\"\n",
    "    assert N_train <= X_train.shape[0]\n",
    "    return X_train[:N_train, :], y_train[:N_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_error(clf, X, y):\n",
    "    return (np.sum(abs(clf.predict(X) - y))) / (2*X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pre_process_mnist_01()\n",
    "\n",
    "clf = SGDClassifier(loss='log', max_iter=1000, \n",
    "                    tol=1e-3,\n",
    "                    penalty='l1', alpha=0.01, \n",
    "                    learning_rate='invscaling', \n",
    "                    power_t=0.5,                \n",
    "                    eta0=0.01,\n",
    "                    verbose=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "test = classification_error(clf, X_test, y_test)\n",
    "train = classification_error(clf, X_train, y_train)\n",
    "print('train: ', train, end='\\t')\n",
    "print('test: ', test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a117df6",
   "metadata": {},
   "source": [
    "### Question 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88cf8865",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGDClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3276/439563585.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malpha_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     clf = SGDClassifier(loss='log', max_iter=1000, tol=1e-3, penalty='l1', alpha=alpha, \n\u001b[0m\u001b[0;32m      8\u001b[0m                         learning_rate='invscaling', power_t=0.5, eta0=0.01)\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SGDClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "mean_list, std_list = [], []\n",
    "alpha_list = np.logspace(-4,-1, 10)\n",
    "\n",
    "\n",
    "for alpha in alpha_list: \n",
    "\n",
    "    clf = SGDClassifier(loss='log', max_iter=1000, tol=1e-3, penalty='l1', alpha=alpha, \n",
    "                        learning_rate='invscaling', power_t=0.5, eta0=0.01)\n",
    "\n",
    "    error_list = []\n",
    "    for _ in range(0, 10): \n",
    "        X_train, y_train = sub_sample(100, X_train, y_train)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        error_list.append(classification_error(clf, X_test, y_test))\n",
    "\n",
    "    mean_list.append(np.mean(error_list))\n",
    "    std_list.append(np.std(error_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3caff84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (15,8))\n",
    "\n",
    "plt.errorbar(alpha_list,mean_list,yerr = std_list, fmt = 'o')\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ae1d4",
   "metadata": {},
   "source": [
    "### Question 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2124dafe",
   "metadata": {},
   "source": [
    "The sole source of randomness comes from the selection of the subset of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623589fb",
   "metadata": {},
   "source": [
    "### Question 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89052340",
   "metadata": {},
   "source": [
    "The optimal value of the parameter alpha is 0.1, it has the lowest test error mean as seen in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3f11ba",
   "metadata": {},
   "source": [
    "### Question 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0aaf6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='log', max_iter=1000, tol=1e-3, alpha=0, \n",
    "                        learning_rate='invscaling', power_t=0.5, eta0=0.01)\n",
    "clf.fit(X_train, y_train)\n",
    "coeffs = clf.coef_.reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb4f9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_reg = SGDClassifier(loss='log', max_iter=1000, tol=1e-3, penalty='l1', alpha=0.05, \n",
    "                       learning_rate='invscaling', power_t=0.5, eta0=0.01)\n",
    "clf_reg.fit(X_train, y_train)\n",
    "coeffs_reg = clf_reg.coef_.reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25e2c803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATwAAAD4CAYAAABxC1oQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeFUlEQVR4nO3de4xcZ5nn8e+vqtt3x3bStuP4EjvBYZOFwTEeJ7sZRkFc1smyOOwuo2R3IcuiNUhEAokVkwFpQSvtCu0OzIA2SmSCNYmWIYsEDB7knZDJgALMJLKTCbnghJiQ2J04duzcfInd7q5n/6hq6Eud9xy7qruq6/w+0pG76jlv1dvV3Y/fc85z3lcRgZlZGVQ63QEzs+nihGdmpeGEZ2al4YRnZqXhhGdmpdE3nW82MDAQa9asmc63NCuV/fv3c+TIEbXyGpXzVgXDpwrtG28evTcitrTyftOppYQnaQvwNaAK3BkRX07tv2bNGn7+85+38pZmlnDNNde0/iIjp+m//EOFdh165M6B1t9w+pxzwpNUBW4D3gcMArsl7YyIX7arc2bWGapUO92FKdHKCG8zsC8ingWQdA+wFXDCM5vR5ITXxErgwJjHg8BVE3eStA3YBrB69eoW3s7MpoWc8JppdmJ00n1qEbEd2A6wceNG38dm1uUkUe2f1eluTIlWEt4gMHbItgp4sbXumFk38Ahvst3AeknrgBeAG4F/15ZemVnn+JB2sogYlnQLcC/1spQdEfFk23pmhbVUdNXFfP6jMwSo0pv3JLRUhxcRu4BdbeqLmXUFj/DMrCx8SGtmpSFR8VVaMyuD+jk8j/DMrAx8SGtm5SEqTnhmVgryIW3Xa7UWLa/maypr3bq63ixqU/faStd6lfYz7zAhKn2+aGFmZeBzeGZWHk54ZlYWAlWd8MysBOQRnpmVhs/hmVmZVPt6MzX05nd1Djo5xZJaLP3Q0MnMWOXkq8m2w0vS0+4fejPdt6rSn9zS2dkFIEdPp4tDXj01koy/ZWEynCx7UU5JTF7JzFTqdMmMJFTpzUnHnPDMbBLl/Ec2UznhmdkkFY/wzKwUhA9pzawc6tND9WbC682J683s3ElUq5VCW7GX0xZJT0vaJ+nWJnFJ+noj/pikjRPiVUn/KOmHrX5rTnhmNokqKrTlvo5UBW4DrgOuAG6SdMWE3a4D1je2bcDtE+KfBva2+j2BE56ZTSDVL1oU2QrYDOyLiGcjYgi4B9g6YZ+twN1R9yCwWNKKel+0CviXwJ3t+N58Dq+oRK3c3lfOJJv+8uXjyfh7L1mSjC8Zfj0Zr83JLkg7OPuiZNtqTq3bhXOSYaqv56y9nqjju+D8i5NN8/6cnj+RrhG8eGH2/+eVN9OfafTNTserOX86lZx4B+v8ijiL7g1I2jPm8faI2D7m8UrgwJjHg8BVE16j2T4rgYPAnwOfA/KqLgtxwjOzSc6iDu9IRGxKvVST5ybWVjfdR9IHgMMR8bCka4t2KMUJz8zGkUS1r20j0EFg7O08q4CJhwVZ+/xb4IOSrgfmAOdJ+j8R8R/OtTPdPa42s45o10ULYDewXtI6SbOAG4GdE/bZCXy0cbX2auD1iDgYEX8SEasiYm2j3d+1kuzAIzwzm0hQadOtZRExLOkW4F6gCuyIiCclfbIRvwPYBVwP7ANOAh9ry5s34YRnZuO0u/A4InZRT2pjn7tjzNcBfCrnNX4C/KTVvjjhmdkEni3FzMpCnjygK0zpjyBnTroXEjVfh0+cTra9etWiZHxuzhUxHX8jGe8/vC8ztmh1qmIA5oy8mYwfu/N/JOOvPPV8Mn7h5sszY3Pf/++TbZefStfKDSxamYxXD7+cGYtqehnC2tz0z0wj6T+d2qy5yTiJ98/7PZ/q+fIEVKpOeJNIeg44BowAwzn1OGY2E3iEl/TuiDjShtcxsy7hc3hmVhLq2RmPWy08DuBHkh6WtK3ZDpK2Sdojac+RIx4ImnW7Nk8e0FVaHeFdExEvSloG3CfpqYh4YOwOjRuJtwNs3Lix0+uTmFkBPqRtIiJebPx7WNL3qU8F80C6lZl1Mwlmte9e2q5yzt+VpPmSFo5+DbwfeKJdHTOzzhCiWim2zTStjPCWA99vnNzsA/4yIv6mLb2aCi2u/XryTHb7ywfmJ9sun5WeL6/v0FPJ+MjhwXR8OPv15y/4TbKtzmSvaQuw4LLLkvG++ekJ82ZfOnFy29+pnD6WbDt84JlkvHoyPc/g8MsvZMb61v3TZFsN9SfjVNJjhepQum8jOTWEKak005YUJGZkMivinBNeRDwLvKONfTGzLiCc8MysJCToc8IzszKQ1LMXLZzwzGyc+iGtE56ZlYTP4ZlZKchXaUsgp2zlrXMS0yjVhpNtK/vT5RW1oVPJuPrTUxm9ufeRzNicnEOTyqILkvH+yzYk45qdngapb/mazNjwc+m1lYcP7U/GTz79ZDJeO5P9c1m88tJkW3Kmh6KW/n1RLb38ZTcbrcPrRU54ZjZJtUcnD3DCM7NxevnWMic8MxvHdXhmVho+h2dmpeKEZ2al4LIUMysNTx5QBjl1eNU3DmbGhp9L14PF/POS8cr5y5PxU3v+Lhl/4YFfZMbWrbw42Zb1m5Ph2rwlyXh1TrpeLfWp9q3NWSrxRHp5ytf2pKfVGj41lBmbu+Ifkm1nb3hXMk4lZ5nG2QvT7RNamRa8HVOK+15aMysVj/DMrBR8Ds/MSsPn8MysPHp4hNebZybN7JwJ0V+pFNoKvZ60RdLTkvZJurVJXJK+3og/Jmlj4/nVkn4saa+kJyV9utXvzSM8MxtHQLVNAzxJVeA24H3AILBb0s6I+OWY3a4D1je2q4DbG/8OA5+NiEcaKyQ+LOm+CW3Pikd4ZjaeoFJRoa2AzcC+iHg2IoaAe4CtE/bZCtwddQ8CiyWtiIiDEfEIQEQcA/YC577cG102wuvkWYPK6RPJ+PCzjycap//fiJH03GhnfvVoMn7k0aeT8aPPvJIZu3ReugawljPv2wvpVRyJ/ouS8b7EUGHWvPRPfPlbEnMQAm/c/cNk/JV9r2bGImc+u5U5cxTO2fjuZLyi9O9E6jci7++gHbV2KfURXuG/xgFJe8Y83h4R28c8XgkcGPN4kProjZx9VgK/LX6VtBa4EnioaMea6aqEZ2bdoVI84R2JiE2JeLMXmpizk/tIWgB8F/hMRKSr0XM44ZnZOO08h0d9tLZ6zONVwItF95HUTz3ZfSsivtdqZ3wOz8zGkURftVJoK2A3sF7SOkmzgBuBnRP22Ql8tHG19mrg9Yg4KEnAN4G9EfHVdnxvHuGZ2STtGuFFxLCkW4B7gSqwIyKelPTJRvwOYBdwPbAPOAl8rNH8GuAjwOOSHm089/mI2HWu/XHCM7NxxFmdw8vVSFC7Jjx3x5ivA/hUk3Y/o83XMp3wzGy8Hr7TwgnPzMZp9wivmzjhNVSOv5yMDyfWGc371agdfy0Zf/3JdOH44V8cSMYv+v3stV+rF1yYbPsm/cn4/3vmUDL+/NF0/eJly7PnhVsyN/3eNyxOr5k7+7w5yXjfnOxf76E30jV+Rx7bl4wvOpFuv/Bd1yfjyfkXc2r4Ur9v7UpTbbxK21VyL7NI2iHpsKQnxjx3vqT7JD3T+Dc9S6SZzRiS6K9WCm0zTZEe/wWwZcJztwL3R8R64P7GYzPrAfVD2mLbTJOb8CLiAWDivUtbgbsaX98F3NDebplZJ1WlQttMc67n8JZHxEGARoHgsqwdJW0DtgGsXr06azcz6xK9fNFiyg/CI2J7RGyKiE0DAwNT/XZm1ipBtVJsm2nOdYR3aHT6FkkrgMPt7JSZdc7oBKC96Fy/q53AzY2vbwZ+0J7umFmnjR7SFtlmmtwRnqRvA9dSn/dqEPgi8GXgO5I+DuwHPjyVnWyLnHVnawfSc86NvJo9iK0uyTyFWX/rN9O1atX+9I9h8bp0PZpa+N/4scPpCe/++h8nTmwx3tCZ9Fx/Q8PZn/uynDo6lqS/r6ET2evOApy3KlEDeFl6HklVq8n4sf3p+sT+Rx9IxqurNyTjHaWZebhaRG7Ci4ibMkLvaXNfzKwL9PJFC99pYWaT9Gi+c8Izs8kqHV1wYeo44ZnZOPWFuDvdi6nhhGdm48mHtGZWEkI+pJ3xcspSUmUnAAyfyQzlTcE0PPjrZPzES0eT8eMHX0/Gl2+8JDMW89MT2dz50+eT8Sf+fm8yvmTF8mT84dPDmbHN/2Rpsu3wU3uS8RhJ/0z75mb/elf601NTLViZvivo2IH078trT/0mGb/gXyXDHecRnpmVxkycCaUIJzwzG+csF+KeUZzwzGySHs13TnhmNlmPVqU44ZnZeFJ9mvde5IRnZpP4ooWZlUaPDvCmN+GJNi8jfjbvnVeHd/JYMl5ZsDi77Uv7k22P7H40GT+2P71EZJ7U9FCnBt6SbHvvX30zGT9zIl0DeGLeomS8VovM2Iqrs5eXBHjtwUeS8bxlGlPTR505mV5mceRMdv0g5NfpvfbrF5LxVpZpzP5E07GifJXWzMpjhq5IVoQTnplN0qP5zgnPzMYbXZe2FznhmdkkvVqW0qv1hWZ2jkZHeEW2Qq8nbZH0tKR9km5tEpekrzfij0naWLTt2XLCM7MJRFXFttxXkqrAbcB1wBXATZKumLDbdcD6xrYNuP0s2p4VJzwzG0+jd1vkbwVsBvZFxLMRMQTcA2ydsM9W4O6oexBY3FjvukjbszKt5/CC1uqEWjqrUEvX4Z05djwZr5zMXs6wljMv26mjbyTjtZH0p9KfmNcNoH9+dj3aPwym6wuPv5Set61v7oJk/PTxV5LxBUuyl0O8fGn6tZUzz3h1zqxkfHaifW0oXWfXvzDdtziTXiIyb+nNbqYIFIX/UgckjZ24cHtEbB/zeCVwYMzjQeCqCa/RbJ+VBduelZn7UzGzqZNTqD/GkYjYlIg3G6dMzKZZ+xRpe1ac8Mxskrw7k87CILB6zONVwMTV3bP2mVWg7VnxOTwzmyCgNlJsy7cbWC9pnaRZwI3Azgn77AQ+2rhaezXwekQcLNj2rHiEZ2bjRZzNIW3OS8WwpFuAe4EqsCMinpT0yUb8DmAXcD2wDzgJfCzVtpX+OOGZ2SRtPKQlInZRT2pjn7tjzNcBfKpo21Y44ZnZZG1MeN3ECc/MJmjfIW23KU/Cq6Xrrt48/Foy3peo+arl1PjlrSsbOe3nLpmbjC98x5WZsb9//tVk29pwup7szaPped0WLl+bjC+6YF5m7Jo15yXbns6pb8z73M6cOJ0ZO2/timTbkUTdZf21TyXjC9ek1+vtakHPJrzcq7SSdkg6LOmJMc99SdILkh5tbNdPbTfNbPoEGhkutM00RcpS/gLY0uT5P4uIDY2tbScVzawLRK3YNsPkHtJGxAOS1k5DX8ysG0TUtx7USuHxLY2pXHZIWpK1k6RtkvZI2nPkyJEW3s7Mpk2PjvDONeHdDlwKbAAOAl/J2jEitkfEpojYNDCQXvjEzLqDolZom2nO6SptRBwa/VrSN4Aftq1HZtZhLksZR9KKxr1uAB8Cnkjtb2YzSERuGddMlZvwJH0buJb6vFeDwBeBayVtoF6x8xzwiXZ0Zipn0c+b36syK/1RnDmZXXd14uDRZNtTr6ZrtuYNpOvsZi9Oz83Ghn+RGXrpvnQd3ayF5yfjJ18+kIwvXHpBMv6v//nFmbEFx9MTX5zKqcMbeiO9tqyq2b9Rx19IrwU854J0jeDc1auS8erCzNPaAETO2rMpqb+TdvwNifbeWtZNilylvanJ0+nVm81sZssp6p6pynOnhZkV1LtlKU54ZjZeD99a5oRnZhMEKutFCzMrIY/wzKwUIopO3z7jlCfhjaSnQUotdQhw+rXs5Q6HjqWnEmrVsne+NRk/EtllLSO11k4+zz4vfXfMslWLkvE/uDhRnnHwoWTbvOmfUmUnkC4HmptXdrJmbfq9Z6dLiWa6vM9+pipPwjOzgjzCM7OyCJzwzKwcIoI4c6bT3ZgSTnhmNoEPac2sLCIIJzwzKw1fpTWzcvAIr+fNW3tJMv7ar7KnSTp5JD1N0anX0tNDXXT1umR87jvfnYw/8Xp2jeHJofQvbu1Muj5x/rI1yfjvr09PD/X2pdn1ake/nV77afCnTyfj1VnpKZaWviO778uufVf6tRelv6+R19NTgmlWuq6zq/kqrZmVhq/Smll5+CqtmZWF76U1szLp1XtpW1mX1sx6UmOEV2RrgaTzJd0n6ZnGv01nmpC0RdLTkvZJunXM8/9L0lON9bG/L2lx3ns64ZnZOBFBDJ8ptLXoVuD+iFgP3N94PI6kKnAbcB1wBXCTpCsa4fuAt0XE7wG/Av4k7w2d8MxsvNGylCke4QFbgbsaX98F3NBkn83Avoh4NiKGgHsa7YiIH0XE6NTMDwLppeTopXN4eTO05iyL13dRuhZu3oXZS+/+5v5nkm1feelEMn7xqXQt3MiytyTj809Xs197YH6y7YVvuyoZX7A4XU/2kXeuTsYrD303M/bGcwczYwDnv3VZMr740pXJ+MK3vyO7Xzl1durrT8bJqV+sLk/XL3b3BOpnddFiQNKeMY+3R8T2gm2Xj65vHREHJTX7ga8ExhbBDgLNfmn/E/B/896wdxKembVHQIwUTnhHImJTVlDS3wIXNgl9oeDrN5vlddystpK+QP3/kG/lvZgTnplNEG27lzYi3psVk3RI0orG6G4FcLjJboPA2MOIVcBvV3CXdDPwAeA9EflrS/ocnplNNj3n8HYCNze+vhn4QZN9dgPrJa2TNAu4sdEOSVuAPwY+GBGF1lnwCM/Mxoug1voV2CK+DHxH0seB/cCHASRdBNwZEddHxLCkW4B7gSqwIyKebLT/38Bs4D5JAA9GxCdTb+iEZ2bjRRAjU194HBFHgfc0ef5F4Poxj3cBk2aaiIj01bwmnPDMbJwIpiXhdYITnplNED17a1l5El4l/a3qvHRd1qK3XZEZu+yG9Hx4+/768WT89efTc6uteXFvMr5+XWZVAJ9718XJtptWL07G81wxMpiMnxr8dWZsyfp0Dd95G65MxvPmpEvV2lXmLUy2PXPgV8k4/bOS4eFLNqfbtyB1KbK1VYh/9yKlHeFJWg3cTb2Wpka9sPBrks6nXui3FngO+KOIeHXqumpm0yEiGBnqzfnwipSlDAOfjYjLgauBTzXuZcu9D87MZqao1QptM03uCK9x68fo7R/HJO2lfrvHVuDaxm53AT+hXhNjZjPZNF2l7YSzOocnaS1wJfAQxe6DQ9I2YBvA6tXpczZm1h16NeEVvtNC0gLgu8BnIuKNou0iYntEbIqITQMDA+fSRzObRhFR3kNaAEn91JPdtyLie42ni9wHZ2YzUK1HR3hFrtIK+CawNyK+OiY0eh/cl8m+D278a9F86oPpEEq/c212ukyh//f+MDO2dHZ6CqXqrPRUQ8OnTifjp5/ak4zPrmQP1Cuvvpxs+4FLNiTjMee8ZHzkicfS7RN/OIs2/7Nk2zx9S9PTQ8Xp7HKhkTfTU3bVcuJ91/ybdPuc6chS05Vd/d9+nGz64H/NXrazLX9ftaA21N0TWJ2rIiO8a4CPAI9LerTx3OfJuA/OzGa2oHfXtChylfZnZP/HMek+ODOb4XyV1szKxAnPzMohoFbWQ1ozK5fAh7RmVhYR1M705r20TnhmNl6ZZ0uZMfLqnnKmh4r+dC3dyKIVmbHq5dnLJAIszVkSMHeFqJzptmPoVGZs5NV0PXh1/5PJeGVuepnH4cR7A1TmzcuMaX669pHllybDMSv7tQEqhxLLZy44P9m2761XJ+O1eUuS8Vak6uymhw9pzawkIkp8p4WZlY1nPDazsqhBbajlJRi7khOemY0ThA9pzawkAqLWltUxuo4TnplNUhtxwjOzEvC6tG0StGkZuSby5gGLvvSyetTS839V3nw9+7Vzavi05m3pt56frtPrO/pcMj789O7M2GuPp5d4XPz2ZBhdtC4Zr+bUGKaWQ4yL08swvlpN1+ktmpWuvTy0YnFm7IU3hpJtN8yfnYzn1n1OoalfpjEIj/DMrBQCRnyV1szKIICaL1qYWSn4kNbMyqRX6/A6d+bVzLpS/SptFNpaIel8SfdJeqbxb9MZGSRtkfS0pH2Sbm0S/y+SQlLuOrBOeGY23jQlPOBW4P6IWA/c33g8jqQqcBtwHXAFcJOkK8bEVwPvo76QWC4nPDMbL4KRMyOFthZtBe5qfH0XcEOTfTYD+yLi2YgYAu5ptBv1Z8DnKFiRU55zeDl1U3lzqzGcXbelM9nrnwJEpM+HjFTSNYJadFE6Pid7zrq5yxYn2w6/kl63tnrVB9LvnfO9VV8ZzA6++nyy7QWz02vi5s1Jt3ROdh3f0rlzk23LLDirOy0GJI1dOHl7RGwv2HZ5RBwEiIiDkpY12WclcGDM40HgKgBJHwReiIhfKGfd6VHlSXhmVszZLdN4JCI2ZQUl/S1wYZPQFwq+frNMFpLmNV7j/QVfB3DCM7Mm2lWWEhHvzYpJOiRpRWN0twJoNj33ILB6zONVwIvApcA6YHR0twp4RNLmiHgp6z19Ds/MxqnPeByFthbtBG5ufH0z8IMm++wG1ktaJ2kWcCOwMyIej4hlEbE2ItZST4wbU8kOPMIzs4kaFy2mwZeB70j6OPWrrB8GkHQRcGdEXB8Rw5JuAe4FqsCOiEgvxJLghGdm40X7DmmTbxNxFHhPk+dfBK4f83gXsCvntdYWeU8nPDMbJ/D0UGZWFlHiCUAblcx3U7+0XKNeZ/M1SV8C/jMwWsj1+cbQsyNyfzw5dXjKrdPL/qhya/jyatVIx3Nff9MHM0Nz3pmuo8uT+/98zuc2snD5ObfN05t/kt2g3JMHDAOfjYhHJC0EHpZ0XyP2ZxHxp1PXPTObbhFQi5ImvEYl9Gg19DFJe6lXP5tZDwpgqEfnwzurYwpJa4ErgYcaT90i6TFJOxIzHWyTtEfSniNHjrTWWzObFiMRhbaZpnDCk7QA+C7wmYh4A7iderXzBuojwK80axcR2yNiU0RsGhjInb3FzDosgJEots00ha7SSuqnnuy+FRHfA4iIQ2Pi3wB+OCU9NLNpFcGMHL0VUeQqrYBvAnsj4qtjnl8xOtMB8CHgianpoplNt5k4eiuiyAjvGuAjwOOSHm0893nqE/FtoD4Cfg74xBT0b9pM6c+3xfKLYhPfNBcdXE7QZqZgZp6fK6LIVdqf0fxvrmM1d2Y2depXaTvdi6nhOy3MbJxSn8Mzs/Ip8zk8MyuRellKb2Y8JzwzG2e0Dq8XOeGZ2TgRvXtrmROemU3iQ1rrmN781bNuFRSYFmyGcsIzswlKXHhsZuXiixZmVhouSzGz0vBVWjMrFR/Smlkp+JDWzEqjly9aKKYxk0t6GXh+zFMDQLcudNGtfevWfoH7dq7a2beLI2JpKy8g6W+o96mIIxGxpZX3m07TmvAmvbm0JyI2dawDCd3at27tF7hv56qb+9ZrPB2umZWGE56ZlUanE972Dr9/Srf2rVv7Be7buermvvWUjp7DMzObTp0e4ZmZTRsnPDMrjY4kPElbJD0taZ+kWzvRhyySnpP0uKRHJe3pcF92SDos6Ykxz50v6T5JzzT+XdJFffuSpBcan92jkq7vUN9WS/qxpL2SnpT06cbzHf3sEv3qis+tDKb9HJ6kKvAr4H3AILAbuCkifjmtHckg6TlgU0R0vEhV0h8Cx4G7I+Jtjef+J/BKRHy58Z/Fkoj44y7p25eA4xHxp9Pdnwl9WwGsiIhHJC0EHgZuAP4jHfzsEv36I7rgcyuDTozwNgP7IuLZiBgC7gG2dqAfXS8iHgBemfD0VuCuxtd3Uf+DmXYZfesKEXEwIh5pfH0M2AuspMOfXaJfNk06kfBWAgfGPB6ku37oAfxI0sOStnW6M00sj4iDUP8DApZ1uD8T3SLpscYhb0cOt8eStBa4EniILvrsJvQLuuxz61WdSHhq8lw31cZcExEbgeuATzUO3ayY24FLgQ3AQeArneyMpAXAd4HPRMQbnezLWE361VWfWy/rRMIbBFaPebwKeLED/WgqIl5s/HsY+D71Q/BucqhxLmj0nNDhDvfntyLiUESMREQN+AYd/Owk9VNPKt+KiO81nu74Z9esX930ufW6TiS83cB6SeskzQJuBHZ2oB+TSJrfOJmMpPnA+4En0q2m3U7g5sbXNwM/6GBfxhlNJg0fokOfnSQB3wT2RsRXx4Q6+tll9atbPrcy6MidFo3L7n8OVIEdEfHfp70TTUi6hPqoDupzBf5lJ/sm6dvAtdSn6jkEfBH4K+A7wBpgP/DhiJj2iwcZfbuW+mFZAM8Bnxg9ZzbNffsD4KfA4/xuxcHPUz9f1rHPLtGvm+iCz60MfGuZmZWG77Qws9JwwjOz0nDCM7PScMIzs9JwwjOz0nDCM7PScMIzs9L4/6HCMCzwW6paAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale = np.abs(clf.coef_).max()\n",
    "plt.imshow(coeffs, cmap=plt.cm.RdBu, vmax=scale, vmin=-scale)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dce0489f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale = np.abs(clf_reg.coef_).max()\n",
    "plt.imshow(coeffs_reg, cmap=plt.cm.RdBu, vmax=scale, vmin=-scale)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c643110",
   "metadata": {},
   "source": [
    "The pattern of coefficients reflect the pixels of the image, we can see that the value is lower (red) when the pixel is assigned to the target 0 and higher (blue) when it is assigned to the target 1.<br>\n",
    "\n",
    "Additionallty, the regularization turns the less informative coefficients to 0 and therefore they are not seen in the second graph.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
